{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam Filtering\n",
    "In this programming assignment, we will be looking at Spam Filtering with a real data set that has a \"label\" for every email - i.e. spam or not spam. We will use logistic regression classifier to solve this assignment and participate in a friendly competition on Kaggle (Details below). The assignment goes from data loading to data inspection to data pre-processing to creating a train/test data set to finally doing machine learning, making predictions and evaluating it. This is typically one part of the \"full pipeline\" in ML modeling/prototyping - So you will get a sampler taste of some \"prototype pipeline\" work that happens in practice! Have fun!! And if you get stuck somewhere - Use discord - Maybe someone has a suggestion that will unblock you.\n",
    "\n",
    "The submission consists of two parts:\n",
    "a) A submission of your complete working code with train/validation data sets + your write-up with insights and your learnings (details on this provided below)\n",
    "b) Evaluation of your best model on the Kaggle evaluation data set - For this you can form a team of 2 - To brainstorm ideas and make your best submission. Include your team name, team members in your submission.\n",
    "\n",
    "Kaggle Starting Point for the competition: https://www.kaggle.com/t/7d2850f5b99a41fba457f2ad7acd0fca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PA 2 - Advanced Introduction to Machine Learning\n",
    "# Blake Downey\n",
    "# Jan 22nd, 2022\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "local_file=\"data/all_emails.csv\"\n",
    "data_set = pd.read_csv(local_file)#,sep=',',index_col=0,header=None,engine='python',error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Inspecting the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1235</td>\n",
       "      <td>Subject: naturally irresistible your corporate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1236</td>\n",
       "      <td>Subject: the stock trading gunslinger  fanny i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1238</td>\n",
       "      <td>Subject: 4 color printing special  request add...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1239</td>\n",
       "      <td>Subject: do not have money , get software cds ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1240</td>\n",
       "      <td>Subject: great nnews  hello , welcome to medzo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                               text  spam\n",
       "0  1235  Subject: naturally irresistible your corporate...     1\n",
       "1  1236  Subject: the stock trading gunslinger  fanny i...     1\n",
       "2  1238  Subject: 4 color printing special  request add...     1\n",
       "3  1239  Subject: do not have money , get software cds ...     1\n",
       "4  1240  Subject: great nnews  hello , welcome to medzo...     1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Print a few lines (i.e. each line is an email and a label) from the data_set containing spam (use a pandas functionality - e.g. getting the top lines)\n",
    "data_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>2603</td>\n",
       "      <td>Subject: hello guys ,  i ' m \" bugging you \" f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>2604</td>\n",
       "      <td>Subject: sacramento weather station  fyi  - - ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>2605</td>\n",
       "      <td>Subject: from the enron india newsdesk - jan 1...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>2606</td>\n",
       "      <td>Subject: re : powerisk 2001 - your invitation ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>2607</td>\n",
       "      <td>Subject: re : resco database and customer capt...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  spam\n",
       "1026  2603  Subject: hello guys ,  i ' m \" bugging you \" f...     0\n",
       "1027  2604  Subject: sacramento weather station  fyi  - - ...     0\n",
       "1028  2605  Subject: from the enron india newsdesk - jan 1...     0\n",
       "1029  2606  Subject: re : powerisk 2001 - your invitation ...     0\n",
       "1030  2607  Subject: re : resco database and customer capt...     0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Print a few lines from data_set that are not spam\n",
    "data_set[data_set['spam'] == 0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>6587</td>\n",
       "      <td>Subject: re : var calibration issues  we are p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4001</th>\n",
       "      <td>6588</td>\n",
       "      <td>Subject: re : carnegie mellon team meeting  un...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4002</th>\n",
       "      <td>6590</td>\n",
       "      <td>Subject: re : statistica &amp; lunch  rick ,  we a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4003</th>\n",
       "      <td>6592</td>\n",
       "      <td>Subject: re : interview with research dept . c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4004</th>\n",
       "      <td>6593</td>\n",
       "      <td>Subject: re : natural gas storage item  vince ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4005</th>\n",
       "      <td>6594</td>\n",
       "      <td>Subject: revised : organizational changes  to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4006</th>\n",
       "      <td>6595</td>\n",
       "      <td>Subject: valuation methodology  we ' ve had a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4007</th>\n",
       "      <td>6596</td>\n",
       "      <td>Subject: again , i should have also sent the f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4008</th>\n",
       "      <td>6597</td>\n",
       "      <td>Subject: registration materials for nfcf  to :...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4009</th>\n",
       "      <td>6599</td>\n",
       "      <td>Subject: latest  vince ,  i appologize for shi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4010</th>\n",
       "      <td>6601</td>\n",
       "      <td>Subject: visit to enron  grant and vince ,  th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  spam\n",
       "4000  6587  Subject: re : var calibration issues  we are p...     0\n",
       "4001  6588  Subject: re : carnegie mellon team meeting  un...     0\n",
       "4002  6590  Subject: re : statistica & lunch  rick ,  we a...     0\n",
       "4003  6592  Subject: re : interview with research dept . c...     0\n",
       "4004  6593  Subject: re : natural gas storage item  vince ...     0\n",
       "4005  6594  Subject: revised : organizational changes  to ...     0\n",
       "4006  6595  Subject: valuation methodology  we ' ve had a ...     0\n",
       "4007  6596  Subject: again , i should have also sent the f...     0\n",
       "4008  6597  Subject: registration materials for nfcf  to :...     0\n",
       "4009  6599  Subject: latest  vince ,  i appologize for shi...     0\n",
       "4010  6601  Subject: visit to enron  grant and vince ,  th...     0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Print the emails between lines 5000 and 5010 in the data set\n",
    "#The dataset is only 4282 lines, So I printed the emails that have ID 5000 - 5010\n",
    "#data_set[(data_set['id']>=5000) & (data_set['id']<5011)]\n",
    "\n",
    "#update: 1/18/22 print lines 4000-4010 \n",
    "data_set.iloc[4000:4011] # 4000 inclusive, 4011 non inclusive, ie range 4000-4010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Data processing step for this HW: \n",
    "Do the following process for all emails in your data set - 1) Tokenize into words 2) Remove stop/filler words and 3) Remove punctuations \n",
    "Below - We have it done for a sample sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "Apply a tokenizer to tokenize the sentences in your email - So your sentence gets broken down to words. We will use a tokenizer from the NLTK library (Natural Language Tool Kit) below for a single sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Subject', ':', 'only', 'our', 'software', 'is', 'guaranteed', '100', '%', 'legal', '.', 'name', '-', 'brand', 'software', 'at', 'low', ',', 'low', ',', 'low', ',', 'low', 'prices', 'everything', 'comes', 'to', 'him', 'who', 'hustles', 'while', 'he', 'waits', '.', 'many', 'would', 'be', 'cowards', 'if', 'they', 'had', 'courage', 'enough', '.']\n"
     ]
    }
   ],
   "source": [
    "# Example Sentence\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "sentence = \"\"\"Subject: only our software is guaranteed 100 % legal . name - brand software at low , low , low , low prices everything comes to him who hustles while he waits . many would be cowards if they had courage enough .\"\"\"\n",
    "sentence_tokenized = word_tokenize(sentence)\n",
    "print(sentence_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words: Remove Stop Words (or Filler words ) using stop words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "filtered_words = [word for word in sentence_tokenized if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuations: Remove punctuations and other special characters from tokens\n",
    "\n",
    "### 3) Exercise: \n",
    "Inspect the resulting list below for any of your emails - Does it look clean and ready to be used for the next step in spam detection? Any other pre-processing steps you can think of or may want to do before spam detection? How about including other NLP features like bi-grams and tri-grams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_words= [word for word in filtered_words if word.isalnum()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean one email: splits the email into tokens, removes stopwords, removes punctuation,\n",
    "# and removes the word 'subject' from each email\n",
    "def clean_email(email):\n",
    "    email = word_tokenize(email)\n",
    "    email = [word.lower() for word in email if word.lower() not in stopwords.words('english')]\n",
    "    email = [word for word in email if word.isalnum()]\n",
    "    email = [word for word in email if not word == 'subject']\n",
    "    return ' '.join(email)\n",
    "\n",
    "# Use the .apply() function of pandas to apply the same function to each entry in df['text'] column\n",
    "data_set['text'] = data_set['text'].apply(clean_email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Train/Validation Split\n",
    "Now for each email in your data set - You have boiled the email down to its essentials - A list of words that are clean and ready for some Machine Learning! Maybe punctuations matter for spam emails!!? \n",
    "If you wish to keep them, you may for your curiosity and see how it impacts metrics (i.e. skip step 3 above). \n",
    "\n",
    "What we will do now is split the data set into train and test set - The train set can have 80% of the data (i.e. emails along with their labels) chosen at random - But with good representation from both spam and not-spam email classes. And the same goes for the test set - Which would have the remaining 20% of the data.\n",
    "Look up python libraries that can do this data split for you automatically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split #splitting train into train and test sets\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, SGDClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# split dataset into train and validate\n",
    "X_train, X_val, y_train, y_val = train_test_split(data_set['text'], data_set['spam'], test_size=.15)\n",
    "\n",
    "#create a vectorizer object with a cap on max_features \n",
    "vectorize = TfidfVectorizer(max_features=10000) #ngram_range=[1,4], #testing with n-grams\n",
    "\n",
    "X_train = vectorize.fit_transform(X_train) # ONLY call .fit_transform() once and on the trainset\n",
    "X_val = vectorize.transform(X_val) # call .transform() to apply the same transform vectorizer to the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Train your model and evaluate on Kaggle\n",
    "Report your train/validation F1-score for your baseline model (starter LR model) and also your best LR model. Also report your insights on what worked and what did not on the Kaggle evaluation. How can your model be improved? Where does your model make mistakes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       494\n",
      "           1       0.99      0.95      0.97       145\n",
      "\n",
      "    accuracy                           0.99       639\n",
      "   macro avg       0.99      0.97      0.98       639\n",
      "weighted avg       0.99      0.99      0.99       639\n",
      "\n",
      "acc:  98.75%\n",
      "train f1:  0.99027\n",
      "validation f1:  0.97183\n"
     ]
    }
   ],
   "source": [
    "# train a vanilla Logistic Regression with the training set\n",
    "model = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "\n",
    "# make a prediction with the validation set\n",
    "y_pred_val = model.predict(X_val)\n",
    "# also get the preds for the training set\n",
    "y_pred_train = model.predict(X_train)\n",
    "\n",
    "# print out the classification report\n",
    "print(classification_report(y_val, y_pred_val))\n",
    "print(f'acc: {accuracy_score(y_val, y_pred_val) * 100: .2f}%')\n",
    "print(f'train f1: {f1_score(y_train, y_pred_train): .5f}')\n",
    "print(f'validation f1: {f1_score(y_val, y_pred_val): .5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       494\n",
      "           1       1.00      0.99      1.00       145\n",
      "\n",
      "    accuracy                           1.00       639\n",
      "   macro avg       1.00      1.00      1.00       639\n",
      "weighted avg       1.00      1.00      1.00       639\n",
      "\n",
      "acc:  99.84%\n",
      "train f1:  0.99027\n",
      "validation f1:  0.99654\n"
     ]
    }
   ],
   "source": [
    "# implement the SGD Classifier!!!\n",
    "model_2 = SGDClassifier().fit(X_train, y_train)\n",
    "\n",
    "y_pred_val = model_2.predict(X_val)\n",
    "# also get the preds for the training set\n",
    "y_pred_train = model.predict(X_train)\n",
    "\n",
    "print(classification_report(y_val, y_pred_val))\n",
    "print(f'acc: {accuracy_score(y_val, y_pred_val) * 100: .2f}%')\n",
    "print(f'train f1: {f1_score(y_train, y_pred_train): .5f}')\n",
    "print(f'validation f1: {f1_score(y_val, y_pred_val): .5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       494\n",
      "           1       0.99      0.97      0.98       145\n",
      "\n",
      "    accuracy                           0.99       639\n",
      "   macro avg       0.99      0.99      0.99       639\n",
      "weighted avg       0.99      0.99      0.99       639\n",
      "\n",
      "acc:  99.22%\n",
      "train f1:  0.99027\n",
      "validation f1:  0.98258\n"
     ]
    }
   ],
   "source": [
    "model_3 = make_pipeline(StandardScaler(with_mean=False), LogisticRegressionCV(max_iter=800)).fit(X_train, y_train)\n",
    "\n",
    "# run model 3 pipeline on the validation set to get predictions\n",
    "y_pred_val = model_3.predict(X_val)\n",
    "# also get the preds for the training set\n",
    "y_pred_train = model.predict(X_train)\n",
    "\n",
    "print(classification_report(y_val, y_pred_val))\n",
    "print(f'acc: {accuracy_score(y_val, y_pred_val) * 100: .2f}%')\n",
    "print(f'train f1: {f1_score(y_train, y_pred_train): .5f}')\n",
    "print(f'validation f1: {f1_score(y_val, y_pred_val): .5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the test set into the notebook\n",
    "df = pd.read_csv(\"data/eval_students_2.csv\")\n",
    "\n",
    "# apply the same clean email function to each of the emails in the test set\n",
    "df['text'] = df['text'].apply(clean_email)\n",
    "# Use the same vectorizer object on the test set. \n",
    "X_test_2 = vectorize.transform(df['text'])\n",
    "\n",
    "# I am not doing anything special for vocab that has not been seen yet by the vectorizer. So new vocab \n",
    "# is simply discarded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1468, 10000)\n",
      "[0 0 1 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# get the predictions\n",
    "y_pred_test_2 = model_2.predict(X_test_2)\n",
    "\n",
    "# print the shape and results \n",
    "print(X_test_2.shape)\n",
    "print(y_pred_test_2)\n",
    "\n",
    "# remove the 'text' column from the data frame\n",
    "df.pop('text')\n",
    "# add a 'spam' column to the data frame that holds the prediction values\n",
    "df['spam'] = y_pred_test_2\n",
    "\n",
    "# write the new dataframe to the disk. \n",
    "df.to_csv('.\\predictions\\DowneyBlake_11.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
